{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ImageCaption.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"y9uyZ4hGylq9","colab_type":"code","colab":{}},"source":["from __future__ import absolute_import, division, print_function, unicode_literals"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"clQyVx8AMx8x","colab_type":"code","colab":{}},"source":["try:\n","  # %tensorflow_version only exists in Colab.\n","  %tensorflow_version 2.x\n","except Exception:\n","  pass\n","import tensorflow as tf\n","\n","# You'll generate plots of attention in order to see which parts of an image\n","# our model focuses on during captioning\n","import matplotlib.pyplot as plt\n","\n","# Scikit-learn includes many helpful utilities\n","from sklearn.model_selection import train_test_split\n","from sklearn.utils import shuffle\n","\n","import re\n","import numpy as np\n","import os\n","import time\n","import json\n","from glob import glob\n","from PIL import Image\n","import pickle"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xic01zGkjiFB","colab_type":"code","colab":{}},"source":["annotation_zip = tf.keras.utils.get_file('captions.zip',\n","                                          cache_subdir=os.path.abspath('.'),\n","                                          origin = 'http://images.cocodataset.org/annotations/annotations_trainval2014.zip',\n","                                          extract = True)\n","annotation_file = os.path.dirname(annotation_zip)+'/annotations/captions_train2014.json'\n","\n","name_of_zip = 'train2014.zip'\n","if not os.path.exists(os.path.abspath('.') + '/' + name_of_zip):\n","  image_zip = tf.keras.utils.get_file(name_of_zip,\n","                                      cache_subdir=os.path.abspath('.'),\n","                                      origin = 'http://images.cocodataset.org/zips/train2014.zip',\n","                                      extract = True)\n","  PATH = os.path.dirname(image_zip)+'/train2014/'\n","else:\n","  PATH = os.path.abspath('.')+'/train2014/'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"AfknDDbNECS5","colab_type":"code","outputId":"ee4d1b79-5a22-42e3-8ae8-81fddae8a2bf","executionInfo":{"status":"ok","timestamp":1566972187195,"user_tz":-330,"elapsed":9704,"user":{"displayName":"Saurav Mishra","photoUrl":"","userId":"11176386989839561620"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# Read the json file\n","with open(annotation_file, 'r') as f:\n","    annotations = json.load(f)\n","\n","# Store captions and image names in vectors\n","all_captions = []\n","all_img_name_vector = []\n","\n","for annot in annotations['annotations']:\n","    caption = '<start> ' + annot['caption'] + ' <end>'\n","    image_id = annot['image_id']\n","    full_coco_image_path = PATH + 'COCO_train2014_' + '%012d.jpg' % (image_id)\n","\n","    all_img_name_vector.append(full_coco_image_path)\n","    all_captions.append(caption)\n","\n","# Shuffle captions and image_names together\n","# Set a random state\n","train_captions, img_name_vector = shuffle(all_captions,\n","                                          all_img_name_vector,\n","                                          random_state=1)\n","\n","# Select the first 30000 captions from the shuffled set\n","num_examples = 30000\n","train_captions = train_captions[:num_examples]\n","img_name_vector = img_name_vector[:num_examples]\n","\n","len(train_captions), len(all_captions)"],"execution_count":102,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(30000, 414113)"]},"metadata":{"tags":[]},"execution_count":102}]},{"cell_type":"code","metadata":{"id":"F5i6q0wNEZ4p","colab_type":"code","colab":{}},"source":["#Preprocessing the image using Inceptionv3\n","\n","from tensorflow.keras.applications.inception_v3 import InceptionV3\n","def load_image(image_path):\n","    img = tf.io.read_file(image_path)\n","    img = tf.image.decode_jpeg(img, channels=3)\n","    img = tf.image.resize(img, (299, 299))\n","    img = tf.keras.applications.inception_v3.preprocess_input(img)\n","    return img, image_path"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cV3qxJ3IcVPP","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":224},"outputId":"2c5a6055-198c-45ca-8ab3-4c3220ea4ed9","executionInfo":{"status":"ok","timestamp":1566972194878,"user_tz":-330,"elapsed":17365,"user":{"displayName":"Saurav Mishra","photoUrl":"","userId":"11176386989839561620"}}},"source":["# Downloading weights\n","\n","!wget --no-check-certificate \\\n","    https://storage.googleapis.com/mledu-datasets/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5 \\\n","    -O /tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5"],"execution_count":104,"outputs":[{"output_type":"stream","text":["--2019-08-28 06:03:09--  https://storage.googleapis.com/mledu-datasets/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n","Resolving storage.googleapis.com (storage.googleapis.com)... 172.217.194.128, 2404:6800:4003:c03::80\n","Connecting to storage.googleapis.com (storage.googleapis.com)|172.217.194.128|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 87910968 (84M) [application/x-hdf]\n","Saving to: ‘/tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5’\n","\n","/tmp/inception_v3_w 100%[===================>]  83.84M  58.0MB/s    in 1.4s    \n","\n","2019-08-28 06:03:11 (58.0 MB/s) - ‘/tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5’ saved [87910968/87910968]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"AmyzfRjXR4JR","colab":{}},"source":["# Initializing InceptionV3 and loading the pretrained Imagenet weights \n","  \n","image_model = tf.keras.applications.inception_v3.InceptionV3(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000)\n","new_input = image_model.input\n","hidden_layer = image_model.layers[-1].output\n","image_features_extract_model = tf.keras.Model(new_input, hidden_layer)\n","\n","\n","# We will try to get unique images\n","encode_train = sorted(set(img_name_vector))\n","\n","\n","\n","image_dataset = tf.data.Dataset.from_tensor_slices(encode_train)\n","image_dataset = image_dataset.map(load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(16)\n","\n","\n","# extract features from all images\n","\n","image_features_extract_model = tf.keras.Model(new_input, hidden_layer)\n","\n","# Here we are extracting features from the images.\n","\n","for img, path in image_dataset:\n","  \n","\n","  image_dataset = tf.data.Dataset.from_tensor_slices(encode_train)\n","image_dataset = image_dataset.map(\n","  load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(16)\n","\n","#We are dumping  inside disk\n","  \n","for bf, p in zip(batch_features, path):\n","   path_of_feature = p.numpy().decode(\"utf-8\")\n","  np.save(path_of_feature, bf.numpy())\n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NwM2vjSCH-Pj","colab_type":"code","colab":{}},"source":["# we will do preprocessing and tockenize our captions\n","\n","# Now we will find the maximum length of any caption in our dataset\n","\n","def calc_max_length(tensor):\n","    return max(len(t) for t in tensor)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"SA_u7ADjKaX1","colab_type":"code","colab":{}},"source":["# We will ony choose the top 5000 words from the vocabulary\n","\n","top_k = 5000\n","tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k,\n","                                                  oov_token=\"<unk>\",\n","                                                  filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\n","\n","# Converting text into sequence of numbers\n","\n","tokenizer.fit_on_texts(train_captions)\n","train_seqs = tokenizer.texts_to_sequences(train_captions)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zvf7h3XlKg1i","colab_type":"code","colab":{}},"source":["tokenizer.word_index['<pad>'] = 0\n","tokenizer.index_word[0] = '<pad>'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0PM1CjiMKmC5","colab_type":"code","colab":{}},"source":["# Creating the tokenized vectors where all the tokens will be store in vector form\n","\n","train_seqs = tokenizer.texts_to_sequences(train_captions)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Indz-6vZNimx","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":85},"outputId":"f3f5712b-675e-4b17-8da0-b7bc11fb81c6","executionInfo":{"status":"ok","timestamp":1566972210207,"user_tz":-330,"elapsed":3086,"user":{"displayName":"Saurav Mishra","photoUrl":"","userId":"11176386989839561620"}}},"source":["# Pading  each vector to the max_length of the captions\n","# If we do not provide a max_length value, pad_sequences calculates it automatically\n","\n","cap_vector = tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding='post')\n","cap_vector[0]"],"execution_count":110,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([  3,   2, 351, 687,   2, 280,   5,   2,  84, 339,   4,   0,   0,\n","         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","         0,   0,   0,   0,   0,   0,   0,   0,   0,   0], dtype=int32)"]},"metadata":{"tags":[]},"execution_count":110}]},{"cell_type":"code","metadata":{"id":"I8CmvvDCN03P","colab_type":"code","colab":{}},"source":["# Calculates the max_length, which is used to store the attention weights\n","\n","max_length = calc_max_length(train_seqs)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lEdu12AMN3aZ","colab_type":"code","colab":{}},"source":["# Now we will apply Train-Test split in 80-20 ratio . \n","\n","img_name_train, img_name_val, cap_train, cap_val = train_test_split(img_name_vector,\n","                                                                    cap_vector,\n","                                                                    test_size=0.2,\n","                                                                    random_state=0)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"aN27tIxROBm_","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":85},"outputId":"1f2cb21d-948e-40df-98f7-680bfb4beb2e","executionInfo":{"status":"ok","timestamp":1566972210210,"user_tz":-330,"elapsed":2449,"user":{"displayName":"Saurav Mishra","photoUrl":"","userId":"11176386989839561620"}}},"source":["# Checking the sample counts\n","print (\"No of Training Images:\",len(img_name_train))\n","print (\"No of Training Caption: \",len(cap_train) )\n","print (\"No of Training Images\",len(img_name_val))\n","print (\"No of Training Caption:\",len(cap_val) )\n"],"execution_count":113,"outputs":[{"output_type":"stream","text":["No of Training Images: 24000\n","No of Training Caption:  24000\n","No of Training Images 6000\n","No of Training Caption: 6000\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"UzVOPyhaOHUV","colab_type":"code","colab":{}},"source":["#Creating tf.data dataset for the training Our Model . This is a way of data pipelining\n","\n","\n","BATCH_SIZE = 64\n","BUFFER_SIZE = 1000\n","embedding_dim = 256\n","units = 512\n","vocab_size = len(tokenizer.word_index) + 1\n","num_steps = len(img_name_train) // BATCH_SIZE\n","\n","# Shape of the vector extracted from InceptionV3 is (64, 2048)\n","# These two variables represent that vector shape\n","\n","features_shape = 2048\n","attention_features_shape = 64"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"30hWNRc-OJfF","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":129},"outputId":"507fa9ab-da53-4cc2-8e27-0a6d13dc8ce9","executionInfo":{"status":"error","timestamp":1566972210213,"user_tz":-330,"elapsed":2179,"user":{"displayName":"Saurav Mishra","photoUrl":"","userId":"11176386989839561620"}}},"source":["# We will load our numpy files.\n","\n","def map_func(img_name, cap):\n","img_tensor = np.load(img_name.decode('utf-8')+'.npy')\n","return img_tensor, cap"],"execution_count":115,"outputs":[{"output_type":"error","ename":"IndentationError","evalue":"ignored","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-115-eab992aaf3d3>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    img_tensor = np.load(img_name.decode('utf-8')+'.npy')\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"]}]},{"cell_type":"code","metadata":{"id":"PWJa8TMTOMNg","colab_type":"code","colab":{}},"source":["dataset = tf.data.Dataset.from_tensor_slices((img_name_train, cap_train))\n","\n","# We will use map to load the numpy files in parallel\n","\n","dataset = dataset.map(lambda item1, item2: tf.numpy_function(\n","          map_func, [item1, item2], [tf.float32, tf.int32]),\n","          num_parallel_calls=tf.data.experimental.AUTOTUNE)\n","\n","# Now we will shuffle and batch.\n","\n","dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n","dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PW1rUqYEo_qe","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pLXp5nJkOUuB","colab_type":"code","colab":{}},"source":["# Herewe extract the features from the lower convolutional layer of InceptionV3 giving us a vector of shape (8, 8, 2048).\n","#We emplies that in the shape of (64, 2048). Vector will pass through the CNN encoder from the last layer.\n","\n","# Here we will use  neural machine translation with attention using BahdanauAttention\n","\n","\n","class BahdanauAttention(tf.keras.Model):\n","  def __init__(self, units):\n","    super(BahdanauAttention, self).__init__()\n","    self.W1 = tf.keras.layers.Dense(units)\n","    self.W2 = tf.keras.layers.Dense(units)\n","    self.V = tf.keras.layers.Dense(1)\n","\n","  def call(self, features, hidden):\n","    # features(CNN_encoder output) shape == (batch_size, 64, embedding_dim)\n","\n","    # hidden shape == (batch_size, hidden_size)\n","    # hidden_with_time_axis shape == (batch_size, 1, hidden_size)\n","    hidden_with_time_axis = tf.expand_dims(hidden, 1)\n","\n","    # score shape == (batch_size, 64, hidden_size)\n","    score = tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis))\n","\n","    # attention_weights shape == (batch_size, 64, 1)\n","    # we get 1 at the last axis because we are applying score to self.V\n","    attention_weights = tf.nn.softmax(self.V(score), axis=1)\n","\n","    # context_vector shape after sum == (batch_size, hidden_size)\n","    context_vector = attention_weights * features\n","    context_vector = tf.reduce_sum(context_vector, axis=1)\n","\n","    return context_vector, attention_weights"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QzFiQlyTOd5J","colab_type":"code","colab":{}},"source":["#Defining the Encoder using VAE which will extract the features fro the images.\n","\n","class CNN_Encoder(tf.keras.Model):\n","    # Since you have already extracted the features and dumped it using pickle\n","    # This encoder passes those features through a Fully connected layer\n","    def __init__(self, embedding_dim):\n","        super(CNN_Encoder, self).__init__()\n","        # shape after fc == (batch_size, 64, embedding_dim)\n","        self.fc = tf.keras.layers.Dense(embedding_dim)\n","\n","    def call(self, x):\n","        x = self.fc(x)\n","        x = tf.nn.relu(x)\n","        return x"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ab-kpuNzOl1b","colab_type":"code","colab":{}},"source":["#Defining the RNN decoder \n","\n","\n","class RNN_Decoder(tf.keras.Model):\n","  def __init__(self, embedding_dim, units, vocab_size):\n","    super(RNN_Decoder, self).__init__()\n","    self.units = units\n","\n","    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n","    self.gru = tf.keras.layers.GRU(self.units,\n","                                   return_sequences=True,\n","                                   return_state=True,\n","                                   recurrent_initializer='glorot_uniform')\n","    self.fc1 = tf.keras.layers.Dense(self.units)\n","    self.fc2 = tf.keras.layers.Dense(vocab_size)\n","\n","    self.attention = BahdanauAttention(self.units)\n","\n","  def call(self, x, features, hidden):\n","    # defining attention as a separate model\n","    context_vector, attention_weights = self.attention(features, hidden)\n","\n","    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n","    x = self.embedding(x)\n","\n","    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n","    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n","\n","    # passing the concatenated vector to the GRU\n","    output, state = self.gru(x)\n","\n","    # shape == (batch_size, max_length, hidden_size)\n","    x = self.fc1(output)\n","\n","    # x shape == (batch_size * max_length, hidden_size)\n","    x = tf.reshape(x, (-1, x.shape[2]))\n","\n","    # output shape == (batch_size * max_length, vocab)\n","    x = self.fc2(x)\n","\n","    return x, state, attention_weights\n","\n","  def reset_state(self, batch_size):\n","    return tf.zeros((batch_size, self.units))\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jqg5zLSeOqrU","colab_type":"code","colab":{}},"source":["encoder = CNN_Encoder(embedding_dim)\n","decoder = RNN_Decoder(embedding_dim, units, vocab_size)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"knFSCxwAO0ls","colab_type":"code","colab":{}},"source":["optimizer = tf.keras.optimizers.Adam()\n","loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n","from_logits=True, reduction='none')\n","\n","def loss_function(real, pred):\n","mask = tf.math.logical_not(tf.math.equal(real, 0))\n","loss_ = loss_object(real, pred)\n","\n","mask = tf.cast(mask, dtype=loss_.dtype)\n","loss_ *= mask\n","\n","return tf.reduce_mean(loss_)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hRsrdhTfO58A","colab_type":"code","colab":{}},"source":["checkpoint_path = \"./checkpoints/train\"\n","ckpt = tf.train.Checkpoint(encoder=encoder,decoder=decoder,optimizer = optimizer)\n","ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ANZLm0LJO6Kv","colab_type":"code","colab":{}},"source":["start_epoch = 0\n","if ckpt_manager.latest_checkpoint:\n","start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"boIw6ldHPMfD","colab_type":"code","colab":{}},"source":["# adding this in a separate cell because if you run the training cell\n","# many times, the loss_plot array will be reset\n","loss_plot = []"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EgD3zoLjPOHU","colab_type":"code","colab":{}},"source":["@tf.function\n","def train_step(img_tensor, target):\n","  loss = 0\n","\n","  # initializing the hidden state for each batch\n","  # because the captions are not related from image to image\n","  hidden = decoder.reset_state(batch_size=target.shape[0])\n","\n","  dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * BATCH_SIZE, 1)\n","\n","  with tf.GradientTape() as tape:\n","      features = encoder(img_tensor)\n","\n","      for i in range(1, target.shape[1]):\n","          # passing the features through the decoder\n","          predictions, hidden, _ = decoder(dec_input, features, hidden)\n","\n","          loss += loss_function(target[:, i], predictions)\n","\n","          # using teacher forcing\n","          dec_input = tf.expand_dims(target[:, i], 1)\n","\n","  total_loss = (loss / int(target.shape[1]))\n","\n","  trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n","\n","  gradients = tape.gradient(loss, trainable_variables)\n","\n","  optimizer.apply_gradients(zip(gradients, trainable_variables))\n","\n","  return loss, total_loss"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"O4gTHEURPXI6","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":129},"outputId":"d911e9e1-afc4-4d17-b351-dde527e031dd","executionInfo":{"status":"error","timestamp":1566972213738,"user_tz":-330,"elapsed":3932,"user":{"displayName":"Saurav Mishra","photoUrl":"","userId":"11176386989839561620"}}},"source":["def evaluate(image):\n","attention_plot = np.zeros((max_length, attention_features_shape))\n","\n","hidden = decoder.reset_state(batch_size=1)\n","\n","temp_input = tf.expand_dims(load_image(image)[0], 0)\n","img_tensor_val = image_features_extract_model(temp_input)\n","img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n","\n","features = encoder(img_tensor_val)\n","\n","dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n","result = []\n","\n","for i in range(max_length):\n","predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n","\n","attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n","\n","predicted_id = tf.argmax(predictions[0]).numpy()\n","result.append(tokenizer.index_word[predicted_id])\n","\n","if tokenizer.index_word[predicted_id] == '<end>':\n","return result, attention_plot\n","\n","dec_input = tf.expand_dims([predicted_id], 0)\n","\n","attention_plot = attention_plot[:len(result), :]\n","return result, attention_plot"],"execution_count":119,"outputs":[{"output_type":"error","ename":"IndentationError","evalue":"ignored","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-119-39cd9f6e0d9c>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    attention_plot = np.zeros((max_length, attention_features_shape))\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"]}]},{"cell_type":"code","metadata":{"id":"-cgX2a6xRNjP","colab_type":"code","colab":{}},"source":["def plot_attention(image, result, attention_plot):\n","temp_image = np.array(Image.open(image))\n","\n","fig = plt.figure(figsize=(10, 10))\n","\n","len_result = len(result)\n","for l in range(len_result):\n","temp_att = np.resize(attention_plot[l], (8, 8))\n","ax = fig.add_subplot(len_result//2, len_result//2, l+1)\n","ax.set_title(result[l])\n","img = ax.imshow(temp_image)\n","ax.imshow(temp_att, cmap='gray', alpha=0.6, extent=img.get_extent())\n","\n","plt.tight_layout()\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lUQE0o_PRP3a","colab_type":"code","colab":{}},"source":["\n","# captions on the validation set\n","rid = np.random.randint(0, len(img_name_val))\n","image = img_name_val[rid]\n","real_caption = ' '.join([tokenizer.index_word[i] for i in cap_val[rid] if i not in [0]])\n","result, attention_plot = evaluate(image)\n","\n","print ('Real Caption:', real_caption)\n","print ('Prediction Caption:', ' '.join(result))\n","plot_attention(image, result, attention_plot)\n","# opening the image\n","Image.open(img_name_val[rid])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3MDzlS4ZEfuz","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"R5pm9UsMAR5w","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KaOy_8ZLGluq","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BbHtIboBPDng","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fpupuEqylbEc","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}